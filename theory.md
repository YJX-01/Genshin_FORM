# 函数导向的圣遗物模型：一种新的模型范式

Function Oriented Relic Model: A New Paradigm for Genshin Artifact Modeling

## 摘要

原神的圣遗物系统是游戏后期占据绝对位置的游戏内容，圣遗物的刷取是对游戏玩家来说一个重要问题。先前的研究通过计算每个部位圣遗物的出现概率来衡量圣遗物的价值并制定了一系列标准，然而这个模型忽略了许多实际问题，首先没有细致地考虑强化策略，同时它使用未经验证的线性函数衡量圣遗物的价值，最后它没有考虑刷取的成本，使得这种模型失去了现实意义。针对以上问题，本文提出了函数导向的圣遗物模型(FORM)，详细讨论了圣遗物的强化策略，通过最小化分数和代价的商来确定限制条件下的最优强化策略，通过线性回归的方式确定线性的赋分函数的系数，从而保证了分数这个线性量可以尽可能多地贴近优化目标，并讨论代价的限制条件，保证了模型更加贴近实际。同时本文总结了圣遗物模型的数学形式，为之后的研究提供了一种新的模型范式。

## 引入

圣遗物强化是在限定条件下的优化问题。这个数学问题由以下几个部分构成：

1. 游戏生成的圣遗物的分布
2. 玩家的强化策略
3. 玩家付出的代价
4. 强化完成的圣遗物具有价值的衡量

玩家在副本中对圣遗物进行刷取，游戏按一定规则生成圣遗物给予玩家。

这些初始的圣遗物具有一定的分布规律。为了获得更高的数值效果，玩家会对圣遗物进行强化。

同时玩家的资源是有限的，玩家需要决定他应该为圣遗物强化付出多少代价，对此他需要进行决策。

这种决策就是强化策略，是在一定代价下取得最佳效果的策略。

在强化行为结束后，我们按照一定的规则对强化完成的圣遗物进行价值衡量，而这种价值标准一定程度上会影响玩家的决策。

### 问题定义

已知游戏生成的圣遗物分布为 $D$，玩家在强化策略 $\mathbb{T}$ 的指导下，在代价为 $L$的情况下完成强化，得到成品圣遗物分布 $D^*$，在玩家的观念中成品圣遗物具有的价值为 $G$，圣遗物的价值是由赋分函数 $f$ 决定的. 同时，玩家的优化目标为 $g$，因此在 $D^*$ 分布下，优化目标呈现 $g(D^*)\mapsto V^*$ 的分布.

问题：选择合适的 $\mathbb{T},\ L,\ f$ 使得 $V^*$ 尽量贴近真实情况

## 强化策略的讨论

### 向量空间和赋分函数的定义

设有 a 个简化的副词条，定义一个圣遗物状态 A，它的套装是 S，位置是 P，它的主词条是 m，它的副词条向量 $$x_0=(s_1,s_2,s_3,\cdots s_a)^T$$此后圣遗物状态 A，它的套装是 S，位置是 P，它的主词条是 m 保持不变，故在下标中省略

初始 3/4 词条数，用 b 表示，也是我们关心的，因为这会影响到强化策略，因此把它吸收进用来计算的向量形式 $$x=(x_0;b)$$所有初始词条数为 b，强化了 i 次的圣遗物状态的副词条向量 x 张成一个向量空间，令其为
$$\mathcal{X}_{b,i}$$
赋分函数可以表示为这样的实函数
$$f: \mathcal{X}_{b,i}\longmapsto \mathbb{R}$$
意为将向量空间上的某个向量映射为一个实数

### 状态对应的概率和状态转移

初状态对应的空间$\mathcal{X}_{b,i}$，末状态对应的空间$\mathcal{X}_{b,i+1}$，不妨令$\left\|\mathcal{X}_{b,i}\right\|=n$, $\left\|\mathcal{X}_{b,i+1}\right\|=n'$

每个状态对应的概率，在$\mathcal{X}_{b,i}$中用$n{\times}1$的列向量 $p=(p_1,p_2,\cdots,p_n)^T$表示，在$\mathcal{X}_{b,i+1}$中用$n'{\times}1$的列向量 $p'=(p_1',p_2',\cdots,p_{n'}')^T$表示

从$\mathcal{X}_{b,i}$转移到$\mathcal{X}_{b,i+1}$就是一次强化。这一过程中，概率发生转移 $$Ap=p'$$ 其中$A$是转移矩阵，每一行表示$\mathcal{X}_{b,i+1}$中的状态是由那些状态线性组合而来，每一列表示$\mathcal{X}_{b,i}$中的状态有多少概率转移到另一个状态

举一个例子 $$\begin{bmatrix}0.5 &0 &0 &0\\0.5 &0.5 &0 &0\\0 &0.5 &0.5 &0\\0 &0 &0.5 &0.5\\ 0 &0 &0 &0.5\end{bmatrix}
\begin{bmatrix}p_1=0.1\\p_2=0.2\\p_3=0.3\\p_4=0.4\end{bmatrix}=
\begin{bmatrix}p_1'=0.05\\p_2'=0.15\\p_3'=0.25\\p_4'=0.35\\p_5'=0.2 \end{bmatrix}$$ $p_1'$只能由$p_1$演变过来，$p_1$会演变为$p_1'$和$p_2'$

注意到$A$由$n$个表示$\mathcal{X}_{b,i}$转移概率的列向量组成，$d_i$表示经过策略选择的第 i 列列向量，而其中$\bar{d_i}$表示未经过策略选择的第 i 列列向量，它们构成$A$的原型$\bar{A}$

$$
\begin{align}A&=\begin{bmatrix}d_1 &d_2 &d_3 &d_4\end{bmatrix}\\
&=\begin{bmatrix}\bar{d_1} &\bar{d_2} &\bar{d_3} &\bar{d_4}\end{bmatrix}\mathrm{diag}(c_1,c_2,c_3,c_4)\\
&=\bar{A}\ \mathrm{diag}(c)\end{align}$$ 我们可以控制$c_1,c_2,c_3,c_4$来控制状态转移，从而控制强化策略

### 平均获得的分数(gain)

对平均得分的定义：在$\mathcal{X}_{b,5}$上，表示概率的$\left\|\mathcal{X}_{b,5}
\right\|{\times}1$列向量为$p_5$，表示概率的$\left\|\mathcal{X}_{b,4}
\right\|{\times}1$列向量为$p_4$，定义$\mathcal{X}_{b,i}$到$\mathcal{X}_{b,i+1}$的转移矩阵为$A_{i}$，赋分函数得出的每个向量空间中的向量的对应分数为$\left\|\mathcal{X}_{b,5}
\right\|{\times}1$的列向量$\omega$
$$\begin{align}G=&\omega^T p_5\\= &\omega^T A_{4} p_4\\=&\cdots\\=&\omega^T A_{4} A_{3} A_{2} A_{1} A_{0} p_0\end{align}$$

为了方便讨论，我们最终的变量应该是一串表示选择的{0,1}序列，用列向量$c_i$表示在$\mathcal{X}_{b,i}$上的选择，$d_{(i,j)}$表示在$A_i$的第j列，则
$$G=\omega^T A_{4} p_4=\sum_i c_{(4,i)} p_{(4,i)} \omega^T \bar{d}_{(4,i)}=\hat{\omega}^T c_4\\
\hat{\omega}_i=p_{(4,i)}\omega^T \bar{d}_{(4,i)}\\
\hat{\omega}=\begin{bmatrix}p_{(4,1)}\omega^T \bar{d}_{(4,1)}&\cdots&p_{(4,n)}\omega^T \bar{d}_{(4,n)}
\end{bmatrix}^T
$$

### 平均付出的代价(loss)

代价和每次的选择相关，令在$\mathcal{X}_{b,i}$上选择的代价为$y_i$，表示概率的列向量$p_i$，则
$$L=\sum_{i=0,1,2,3,4}y_i p_i^T c_i$$
注意到第 i 代的概率依赖于第 0,1,..i-1 代的概率，因此我们在讨论第 i 代是将 L 划分为两个部分
$$L=\sum_{i=0,...k-1}y_i p_i^T c_i+\sum_{i=k,...4}y_i p_i^T c_i=\sum_{i=0,...k-1}y_i p_i^T c_i+\hat{p}^T c_k$$
讨论 k 不同时的情况：

$$
\begin{align}
\hat{p}_{k=4,i}&=p_{(4,i)}\left(y_4\right)\\
\hat{p}_{k=3,i}&=p_{(3,i)}\left(y_3+y_4c_4^T\bar{d}_{(3,i)}\right)\\
\hat{p}_{k=2,i}&=p_{(2,i)}\left(y_2+y_3c_3^T\bar{d}_{(2,i)}+y_4c_4^TA_3\bar{d}_{(2,i)}\right)\\
\hat{p}_{k=1,i}&=p_{(1,i)}\left(y_1+y_2c_2^T\bar{d}_{(1,i)}+y_3c_3^TA_2\bar{d}_{(1,i)}+y_4c_4^T
A_3A_2\bar{d}_{(1,i)}\right)\\
\hat{p}_{k=0,i}&=p_{(0,i)}\left(y_0+y_1c_1^T\bar{d}_{(0,i)}+y_2c_2^TA_1\bar{d}_{(0,i)}+y_3c_3^T
A_2A_1\bar{d}_{(0,i)}+y_4c_4^TA_3A_2A_1\bar{d}_{(0,i)}\right)
\end{align}
$$

性价比(R=gain/loss)

性价比定义为：
$$R=\frac{G}{L}=\frac{\hat{\omega}^T c_4}{\sum_i y_i p_i^T c_i}$$
然而问题在于$\hat{\omega}$依然是关于$c_i$的函数，这使得问题变得非线性，难以解答

但我们先讨论局部最优的问题：
$$R=\frac{G}{L}=\frac{\hat{\omega}^T c_4}{\sum_{i=0,1,2,3} y_i p_i^T c_i+y_4p_4^T c_4}=\frac{\hat{\omega}^T c_4}{\hat{y}+\hat{p}^T c_4}$$

这里只优化$c_4$:
$$\nabla R=\frac{1}{L}(\hat{\omega}-y_4Rp_4)$$

$$[\nabla^2 R]_{ij}=\frac{y_4}{L^2}(2y_4 R p_{4i} p_{4j}-\hat{\omega}_i p_{4j}-\hat{\omega}_j p_{4i})$$
我们难以判断$\nabla^2 R$的正定性

但是观察上式，我们可以利用糖水不等式找到局部极大值：

将$w=\frac{\hat{\omega}_i}{\hat{p}}$从大到小进行排序，得到$w_1,w_2,\cdots,w_n$。全部不选择时 R=0，如果我们要选择其中 k 个使得 R 尽量大，那必定是选择排序中前 k 大的元素。易证，随着 k 增大，R 先增大后减小，所以 R 有一个最大值，对应选择了前 k 个元素，令它们的集合为$w^*$

k 的条件如下，当$\hat{y}$减小时 k 会相对减小，从而使选择更少：$$\frac{\sum_i^k \hat{\omega}_i}{\hat{y}+\sum_i^k \hat{p}}>\frac{\hat{\omega}_{k+1}}{\hat{p}_{k+1}}$$

笔者设想了一种方法来解决这一问题：

将所有$c_i$的每种状态都选择 1 作为初始状态，这样可以算得$\hat{y},\hat{\omega}$

从$c_0$层开始，因为$\hat{y}=0$，这时如果直接按原来算法计算，则 k=1，只会选取最优的唯一状态，这显然不符合我们的期待。所以，我们需要人为规定最底层的选取规则，我们规定代价 L 小于一个阈值，在达到阈值前，w 都可以被选择

其他层，在利用糖水不等式性质求得最优$c_{i-1}$之后将其固定，之后继续计算下一层 ，更新$p_i$，再调整其他$c_i$

例如确定了$c_2$之后，计算$c_3$这一层，同样是利用糖水不等式性质
$$R=\frac{G}{L}=\frac{\hat{\omega}^T c_3}{\sum_{i=0,1,2} y_i p_i^T c_i+y_3p_3^T c_3+y_4p_4^Tc_4}=\frac{\hat{\omega}^T c_3}{\hat{y}+\hat{p}^T c_3}$$
$$G=\omega^T A_{4}A_{3}p_3=\omega^T A^* p_3=\hat{\omega}^T c_3$$
$$\hat{\omega}=\begin{bmatrix}p_{(3,1)}\omega^T d_{(A^*,1)}&\cdots&p_{(3,n)}\omega^T d_{(A^*,n)}\end{bmatrix}^T$$
$$\hat{p}=\begin{bmatrix}y_3p_{(3,1)}+y_4p_{(3,1)}c_4^Td_{(3,1)}&\cdots&y_3p_{(3,n)}+y_4p_{(3,n)}c_4^Td_{(3,n)}\end{bmatrix}^T$$

**其他层依次类推，不断迭代，直到收敛**

_这种策略，暂时无法说明可以收敛，它可能不是最优的，但是一定是足够优的_

## 赋分函数的选择

### 定义

赋分函数和优化目标是两个不同的概念。优化目标顾名思义，是我们最终想要优化的目标，例如伤害量、治疗量、护盾量、充能效率、元素精通、攻击力等。赋分函数，是指导强化策略的函数，是符合人类直觉和理解能力的一种衡量标准，是强化完成的圣遗物具有价值的衡量。

### 特性

1. 理性

假设模型中的玩家是理性人，会做出数学上最优的决策，赋分函数是理性的产物。

2. 盲目性

现实中，当前玩家选择圣遗物组合可能并不是最优的组合。同时，绝大多数玩家并不会记忆当前组合的全部信息，也不会在强化的同时观察最后整体分数的变化趋势。

上述特点导致了强化过程中的盲目性，这具体指代在强化时只关注当前正在强化的圣遗物，而不应时刻关注其他位置或已经强化完成的圣遗物。

3. 可计算性

人类善于计算线性的函数，而难以对非线性函数产生直观的概念。因此，赋分函数应该被设计为线性的以方便人理解和使用。

### 局部性和全局性

最终的分数涉及五个位置，而赋分函数只关注局部单一位置，为了平衡局部性和全局性，我们使用以下的方法。

设经过强化后，位置为 $P_i$ 的圣遗物集合为 $\mathbb{A}_i$，由于盲目性，在讨论 $\mathbb{A}_k$ 时，我们对 $\mathbb{B}=\{\mathbb{A}_i|i\ne k\}$ 只形成一个整体印象

为方便计算，我们取每个位置对应圣遗物集合状态的平均值作为讨论 $\mathbb{A}_k$ 时的参考

$$\hat{x}_i=\mathbb{E}(\mathbb{A}_i)=\sum_{x_{ij}\in \mathbb{A}_i}^\mathrm{elementwise}x_{ij}$$

$$\hat{x}=\sum_{i\ne k}\hat{x}_i$$

### 线性回归

为了满足可计算性，同时使得这个线性函数更加合理，即和非线性的优化目标偏差最小，我们使用线性回归的方法计算赋分函数。

$$f=\omega^Tx+b$$

$$\min\ \sum_{x_{kj}=x_{kj}'+\hat{x},\ x_{kj}'\in \mathbb{A}_k}\frac{1}{2}p(x_j)(\omega^Tx_j+b-y_j)^2$$

### 设定阈值

对某些优化目标，例如元素充能，我们不希望它们超过一定的阈值，因此需要合适的函数限制其突破阈值的行为

$$
f_i=\left\{\begin{align}\begin{matrix}ax_i &,x_i<t
 \\ \frac{a(x_i-t)}{\exp((x_i-t)/c)}+at&,x_i\ge t
\end{matrix}\end{align}\right.
$$

该函数的特性是在 $x_i=t$ 时依然连续，在 $t\le x_i\le t+c$ 时依然增长，可是在 $x_i\ge t+c$ 后下降，最后下降到 $at+b$

这符合刚刚超出阈值时有微小的收益，超出阈值过多收益反而为负的理解。

## 代价的限制

### 从副本中获得的资源

经计算从副本中获得的平均圣遗物经验 $\mathrm{EXP}$ 为

$$\mathrm{EXP}=1.065\times3780+2.485\times2520+3.55\times1260=14760.9$$

为了使得强化的行为可持续，我们需要保证每个部位代价的加权平均小于从副本中获得的资源:

$$\sum_i p_iL_i \le \mathrm{EXP}$$

### 选择合适的代价的原则

代价 L 过大的部位应当适当降低代价，性价比 R 较高的部位应该付出更多的代价以获得更高的分数

## 实验

### 初始化

#### 确定优化目标

本文以冰属性弓箭角色<甘雨>为例进行实验，等级为<90>级，命座为<0>命，天赋等级 <10/8/10>，武器使用<90>级<一>精<阿莫斯之弓>，使用<流浪大地的乐团>套装，圣遗物主词条为<攻击>-<冰伤>-<暴击>

定义伤害是关于圣遗物的函数为 $D(x)$，它由六个乘数（即常说的乘区）连乘得到，基础乘数 $M_{bas}$，增伤乘数 $M_{bns}$，暴击乘数 $M_{crt}$，反应乘数 $M_{amp}$，抗性乘数 $M_{res}$，防御乘数 $M_{def}$

$$D(x) = M_{bas} {\times}M_{bns} {\times}M_{crt} {\times}M_{amp} {\times}M_{res} {\times}M_{def}$$

抗性乘数 $M_{res}$，防御乘数 $M_{def}$，与$x$无关

$$D(x) = K {\times}M_{bas} {\times}M_{bns} {\times}M_{crt} {\times}M_{amp}$$

在本实验中:

$$\begin{align*}M_{bas}&=[(335+608)\times(1+46.6\%+49.6\%+\mathrm{ATK\%})+311+\mathrm{ATK})]\times622\%\\M_{crt}&=1+(5\%+33.1\%+20\%+\mathrm{CR})\times(50\%+38.4\%+\mathrm{CD})\\M_{bns}&=\mathrm{const}=1+46.6\%+35\%+12\%+8\%\times5=2.336\\M_{amp}&=\mathrm{const}=1\\K&=\mathrm{const}=0.5\times0.9=0.45\end{align*}$$

$$D(x)=1.0512{\times}M_{bas}{\times}M_{crt}$$

为了方便表示还可以把基础乘数中的表示技能倍率的乘数单独拿出来，故

$$D(x)=6.538464{\times}M_{bas}'{\times}M_{crt}=K{\times}g(x)$$

$$\begin{align*} g(x)=&\left[(335+608)\times(1+46.6\%+49.6\%+\mathrm{ATK\%})+311+\mathrm{ATK}\right]\times\\&\left[1+(5\%+33.1\%+20\%+\mathrm{CR})\times(50\%+38.4\%+\mathrm{CD})\right]\end{align*}$$

#### 确定代价

目前并没有成熟的理论来确定合适的代价，因此这一步只能凭经验进行

| 初始词条数 | 花     | 羽     | 沙     | 杯    | 冠     |
| ---------- | ------ | ------ | ------ | ----- | ------ |
| 4          | 60,000 | 60,000 | 29,000 | 9,000 | 11,000 |
| 3          | 45,000 | 45,000 | 27,000 | 8,000 | 9,000  |

### 训练模型

$\hat{x}$ 同样需要初始化，但在没有计算前我们对其一无所知，只能凭经验设置。

```python
sub = ['ATK', 'ATK_PER', 'DEF', 'DEF_PER', 'HP', 'HP_PER', 'ER', 'EM', 'CR', 'CD']
ix = np.array([[3, 4, 3, 3, 3, 3, 2, 2, 4, 5],
               [4, 4, 3, 3, 3, 3, 2, 2, 4, 5],
               [3, 5, 3, 3, 3, 3, 2, 2, 4, 5],
               [3, 4, 3, 3, 3, 3, 2, 2, 4, 5],
               [3, 4, 3, 3, 3, 3, 2, 2, 5, 5]])
```

通过迭代法进行训练时，每个位置对应的 $\hat{x}$ 会逐步收敛，所以对应 $f$也会收敛。

同时，经过迭代，强化策略 $\mathbb{T}$ 收敛到一个稳定的状态。至此全部参数都已训练完成。

### 信息输出

#### 玩家获得的资源

每个版本时长为 42 天，我们计算玩家花费在刷本上的体力，需要计入周本的消耗以及尘歌壶和纪行的奖励:

$$42{\times}180_{(daily)}{-}6{\times}90_{(weekly)}{+}[6_{(pot)}{+}5_{(battle)}]{\times}60_{(resin)}{=}7680$$

玩家可获得的五星圣遗物数量

$$7680{\div}20{\times}1.065=408.96$$

针对单个套装，每个部位每个版本可以进行的抽取数为

$$408.96{\times}0.5{\times}0.2=40.896\approx41$$

同时其他途径体力的消耗也不可忽略，为方便讨论，在理想情况下，单个套装每个版本每个部位抽取次数定为 **40** 次

#### "毕业标准"

小毕业：刷取 1 个版本，即对每个部位进行 40 次抽取

大毕业：刷取 5 个版本，即对每个部位进行 200 次抽取

#### "散件"和"2+2"

本模型未能解决散件搭配的问题，因为在本模型下没有记录玩家的历史圣遗物信息。一般而言，<空之杯>部位是散件部位，所以该部位的抽取数应该乘以 2，因为刷本得到的 2 个套装都可以强化这个部位。

本模型也未能解决如何衡量 "2+2" 搭配的问题。对此本文提出一个简易的应对措施，因为 "2+2" 中的部分圣遗物是玩家仓库中的圣遗物，这相当于玩家对这些部位提前进行了刷取，故我们对每个部位的抽取数加上 20，作为 "2+2" 问题的解决方法。

#### 成果展示

![stg](./graph/stg.jpeg)

![low](./graph/low.jpg)

![high](./graph/high.jpg)

优化目标 g 的平均值，在小毕业标准下 g=5406，大毕业标准下 g=6354. 对应伤害值 D 的平均值，在小毕业标准下 35347，大毕业标准下 41545. 对应有效词条数量:

```python
Low = {'ATK': 3.772424587275265,
       'ATK_PER': 5.613025569934037,
       'CR': 7.278480583272985,
       'CD': 8.415503226839085}
High = {'ATK': 3.372691789181934,
        'ATK_PER': 7.397317191027398,
        'CR': 10.141187784040165,
        'CD': 11.239860919635195}
```

| 标准   | $E(g)$ | $E(D)$ | ATK  | ATK% | CR    | CD    | 总有效词条数 |
| ------ | ------ | ------ | ---- | ---- | ----- | ----- | ------------ |
| 小毕业 | 5406   | 35347  | 3.77 | 5.61 | 7.28  | 8.41  | 21.30        |
| 大毕业 | 6354   | 41545  | 3.37 | 7.40 | 10.14 | 11.24 | 28.78        |

回顾参考文献【4】，在"24 词条标准"下同样配置的甘雨分配了 6 条<攻击>，4.32 条<暴击>，13.08 条<暴伤>，最终的 g=5533，对应 D=36178（这里没有算双冰 15%暴击加成），相较于本文模型的小毕业标准偏高，其原因在参考文献【2】中有详细的讨论。

同时注意到 D 不是一个线性函数，也不具有凸性或凹性，故只能说明

$$D(E(x)){\ne}E(D(x))$$

**这表明参考面板和参考伤害是有一定差别的**

对此我们给出一个甘雨的参考伤害分布表

![rank](./graph/rank.jpg)

## 总结

本文提出了函数导向的圣遗物模型(FORM)，通过对强化策略的制定、赋分函数的制定和代价的限制，训练出较为贴合现实的模型。同时本文总结了圣遗物模型的数学形式，为之后的研究提供了一种规范良好的模型范式。通过实验，模型展示了丰富的数据输出，由于良好的数学定义，模型具有较好的可解释性。由于本模型使用的参数较少，由于各参数可以结合实际定义，所以具有很强的可拓展性，可以大规模运用于圣遗物评分、伤害评分、圣遗物潜力计算等场景。

## 参考文献

[1] [圣遗物数学模型研究【1】——模型的建立](https://bbs.mihoyo.com/ys/article/24497190);作者:Moonxu; 2022-6-22

[2] [圣遗物数学模型研究【2】——《体力价值论》的错误、重做与讨论](https://bbs.mihoyo.com/ys/article/24583412);作者:Moonxu; 2022-6-24

[3] [圣遗物数学模型研究【3】——FORM 模型、强化策略的研究](https://bbs.mihoyo.com/ys/article/25236564);作者:Moonxu; 2022-7-8

[4] [24 词条《全角色参考面板》标准拟定与结论导出](https://bbs.nga.cn/read.php?tid=29797262);作者:bluemushoom; 2021-12-10
